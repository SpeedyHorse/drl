{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress as ip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "# train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "import os\n",
    "import csv\n",
    "# ENN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "# K-Means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# WGAN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "DIR_PATH = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_files(dir_name_list):\n",
    "    for _dir in dir_name_list:\n",
    "        paths = glob(f\"{DIR_PATH}/raw/*.csv\")\n",
    "        \n",
    "        for path in paths:\n",
    "            print(path)\n",
    "            filename = os.path.basename(path)\n",
    "            read_write(path, f\"{DIR_PATH}/raw_after/{_dir[0]}/{filename}\", _dir[2])\n",
    "\n",
    "\n",
    "def read_write(path, write_path, labels=None):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as inputs, open(write_path, \"w\", newline=\"\") as outputs:\n",
    "        reader = csv.reader(inputs)\n",
    "        writer = csv.writer(outputs)\n",
    "\n",
    "        headers = next(reader)\n",
    "        if not labels:\n",
    "            headers = [ head.strip() for head in headers ]\n",
    "            writer.writerow(headers)\n",
    "        else:\n",
    "            writer.writerow(labels)\n",
    "\n",
    "        for row in reader:\n",
    "            if row == headers:\n",
    "                break\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961524b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw/cicids2017/DDoS-Friday-WorkingHours-Afternoon.pcap_ISCX.csv\n",
      "./raw/cicids2017/WebAttacks-Thursday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "./raw/cicids2017/Benign-Monday-WorkingHours.pcap_ISCX.csv\n",
      "./raw/cicids2017/Bruteforce-Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "./raw/cicids2017/Botnet-Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "./raw/cicids2017/Portscan-Friday-WorkingHours-Afternoon.pcap_ISCX.csv\n",
      "./raw/cicids2017/DoS-Wednesday-WorkingHours.pcap_ISCX.csv\n",
      "./raw/cicids2017/Infiltration-Thursday-WorkingHours-Afternoon.pcap_ISCX.csv\n",
      "./raw/csecicids2018/02-14-2018.csv\n",
      "./raw/csecicids2018/02-15-2018.csv\n",
      "./raw/csecicids2018/02-20-2018.csv\n",
      "./raw/csecicids2018/02-21-2018.csv\n",
      "./raw/csecicids2018/03-01-2018.csv\n",
      "./raw/csecicids2018/03-02-2018.csv\n",
      "./raw/csecicids2018/02-23-2018.csv\n",
      "./raw/csecicids2018/02-22-2018.csv\n",
      "./raw/csecicids2018/02-16-2018.csv\n",
      "./raw/csecicids2018/02-28-2018.csv\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    [\"cicids2017\", \"BENIGN\", None],\n",
    "]\n",
    "\n",
    "get_csv_files(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050cdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BENIGN' 'DDoS' 'Web Attack Brute Force' 'Web Attack XSS'\n",
      " 'Web Attack Sql Injection' 'FTP-Patator' 'SSH-Patator' 'Bot' 'PortScan'\n",
      " 'DoS slowloris' 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye'\n",
      " 'Heartbleed' 'Infiltration']\n"
     ]
    }
   ],
   "source": [
    "files = glob(f\"{DIR_PATH}/raw_after/cicids2017/*.csv\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    if \"label_\" in file:\n",
    "        continue\n",
    "    tmp = pd.read_csv(file)\n",
    "    df = pd.concat([df, tmp])\n",
    "\n",
    "print(df[\"Label\"].unique())\n",
    "\n",
    "DROP_COLUMNS = [\n",
    "    \"Flow ID\",\n",
    "    \"Source IP\",\n",
    "    \"Source Port\",\n",
    "    \"Bwd PSH Flags\",\n",
    "    \"Bwd URG Flags\",\n",
    "    \"Fwd Avg Bytes/Bulk\",\n",
    "    \"Fwd Avg Packets/Bulk\",\n",
    "    \"Fwd Avg Bulk Rate\",\n",
    "    \"Bwd Avg Bytes/Bulk\",\n",
    "]\n",
    "print(len(df))\n",
    "df = df.drop(columns=DROP_COLUMNS).replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna(how=\"any\").dropna(how=\"all\", axis=1).drop_duplicates()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ababc1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.10.5 3232238085\n",
      "192.168.10.16 3232238096\n",
      "192.168.10.8 3232238088\n",
      "192.168.10.25 3232238105\n",
      "192.168.10.9 3232238089\n"
     ]
    }
   ],
   "source": [
    "for value in df[\"Destination IP\"].unique()[:5]:\n",
    "    ip_int = int(ip.IPv4Address(value))\n",
    "    print(value, ip_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f545600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7/2017 3:30 | 1499365800.0\n",
      "7/7/2017 3:31 | 1499365860.0\n",
      "7/7/2017 3:32 | 1499365920.0\n",
      "7/7/2017 3:33 | 1499365980.0\n",
      "7/7/2017 3:34 | 1499366040.0\n"
     ]
    }
   ],
   "source": [
    "DATE_FORMAT = \"%m/%d/%Y %H:%M\"\n",
    "for value in df[\"Timestamp\"].unique()[:5]:\n",
    "    dt = datetime.strptime(value, DATE_FORMAT)\n",
    "    dt = dt.timestamp()\n",
    "    print(value, \"|\", dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp -> int\n",
    "df[\"continuous_time\"] = df[\"Timestamp\"].apply(\n",
    "    lambda x: datetime.strptime(x, DATE_FORMAT).timestamp()\n",
    ")\n",
    "df = df.drop(columns=[\"Timestamp\"])\n",
    "# rename: continuous_time -> timestamp\n",
    "df = df.rename(columns={\"continuous_time\": \"Timestamp\"})\n",
    "\n",
    "# Destination IP -> int\n",
    "df[\"destination_ip\"] = df[\"Destination IP\"].apply(\n",
    "    lambda x: int(ip.IPv4Address(x))\n",
    ")\n",
    "df = df.drop(columns=[\"Destination IP\"])\n",
    "# rename: destination_ip -> Destination IP\n",
    "df = df.rename(columns={\"destination_ip\": \"Destination IP\"})\n",
    "\n",
    "\n",
    "print(\"wgan running?: \", end=\"\")\n",
    "result = input()\n",
    "\n",
    "if result != \"y\":\n",
    "    counts = df[\"Label\"].value_counts()\n",
    "    for (label, count) in counts.items():\n",
    "        label = label.strip().replace(\" \", \"_\")\n",
    "        df[df[\"Label\"] == label].to_csv(f\"{DIR_PATH}/raw_after/cicids2017/label_{label}.csv\", index=False)\n",
    "\n",
    "    print(\"next: wgan running\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f74b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(dataframe):\n",
    "    for column in dataframe.columns:\n",
    "        # print(dataframe[column].unique())\n",
    "        if len(dataframe[column].unique()) <= 1:\n",
    "            # print(f\"Column {column} has only one unique value. Skipping normalization.\")\n",
    "            continue\n",
    "        dataframe[column] = (\n",
    "            dataframe[column] - dataframe[column].min()\n",
    "        ) / (dataframe[column].max() - dataframe[column].min())\n",
    "    return dataframe\n",
    "\n",
    "def normalize(dataframe):\n",
    "    categorical_frame = dataframe[[\"Destination Port\", \"Protocol\", \"Label\"]]\n",
    "    other_frame = dataframe.drop(columns=[\"Destination Port\", \"Protocol\", \"Label\"])\n",
    "\n",
    "    # normalize\n",
    "    normalized_frame = min_max(other_frame)\n",
    "    normalized_frame = normalized_frame.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    tmp = pd.concat(\n",
    "        [normalized_frame, categorical_frame], axis=1\n",
    "    )\n",
    "    return tmp\n",
    "\n",
    "def under_sampling(dataframe, size):\n",
    "    CLUSTER_SIZE = 1\n",
    "\n",
    "    # normalize\n",
    "    normalized_frame = normalize(dataframe)\n",
    "    print(\"normalized_frame\", normalized_frame.shape)\n",
    "    normalized_frame = normalized_frame.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    continuous_frame = normalized_frame.drop(columns=[\"Destination Port\", \"Protocol\", \"Label\"])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=CLUSTER_SIZE, random_state=0)\n",
    "    print(\"k-means: start\")\n",
    "    cluster_labels = kmeans.fit_predict(continuous_frame)\n",
    "    print(\"k-means: end\")\n",
    "\n",
    "    i = cluster_labels[0]\n",
    "    cluster_indices = np.where(cluster_labels == i)[0]\n",
    "    distances = np.linalg.norm(\n",
    "        continuous_frame.iloc[cluster_indices] - kmeans.cluster_centers_[i], axis=1\n",
    "    )\n",
    "    closest_indices = cluster_indices[np.argsort(distances)[:size]]\n",
    "\n",
    "    return normalized_frame.iloc[closest_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DIM = 74\n",
    "LATENT_DIM = 100\n",
    "\n",
    "class FlowDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data_frame = dataframe\n",
    "        \n",
    "        # self.features = self.data_frame.drop(columns=[\"Label\"])\n",
    "        self.features = self.features.select_dtypes(include=[np.number])\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features.iloc[idx].values.astype(np.float32)\n",
    "        return torch.tensor(features)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(LATENT_DIM, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, FEATURE_DIM),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(FEATURE_DIM, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "critic = Critic()\n",
    "generator_optimizer = optim.RMSprop(generator.parameters(), lr=0.0001)\n",
    "critic_optimizer = optim.RMSprop(critic.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "def over_sampling(dataframe):\n",
    "    dataframe = normalize(dataframe)\n",
    "    size = 20000 - len(dataframe)\n",
    "    label = dataframe[\"Label\"].unique()[0]\n",
    "    label = label.replace(\" \", \"_\")\n",
    "    \n",
    "    checkpoint = torch.load(f\"{DIR_PATH}/wgan/wgan_cicflowmeter_{label}.pth\")\n",
    "    generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
    "    critic.load_state_dict(checkpoint[\"critic_state_dict\"])\n",
    "    generator_optimizer.load_state_dict(checkpoint[\"generator_optimizer_state_dict\"])\n",
    "    critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer_state_dict\"])\n",
    "    generator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(size, LATENT_DIM)\n",
    "        generated_data = generator(z).numpy()\n",
    "\n",
    "    generated_df = pd.DataFrame(generated_data, columns=dataframe.columns[:-1])\n",
    "\n",
    "    return generated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BENIGN 2254674\n",
      "normalized_frame (2254674, 75)\n",
      "k-means: start\n",
      "k-means: end\n",
      "(20000, 75)\n",
      "----------------------------------------\n",
      "DoS Hulk 178179\n",
      "normalized_frame (178179, 75)\n",
      "k-means: start\n",
      "k-means: end\n",
      "(20000, 75)\n",
      "----------------------------------------\n",
      "DDoS 128023\n",
      "normalized_frame (128023, 75)\n",
      "k-means: start\n",
      "k-means: end\n",
      "(20000, 75)\n",
      "----------------------------------------\n",
      "PortScan 119922\n",
      "normalized_frame (119922, 75)\n",
      "k-means: start\n",
      "k-means: end\n",
      "(20000, 75)\n",
      "----------------------------------------\n",
      "DoS GoldenEye 10286\n",
      "(10286, 75)\n",
      "----------------------------------------\n",
      "FTP-Patator 6878\n",
      "(6878, 75)\n",
      "----------------------------------------\n",
      "DoS slowloris 5692\n",
      "(5692, 75)\n",
      "----------------------------------------\n",
      "DoS Slowhttptest 5263\n",
      "(5263, 75)\n",
      "----------------------------------------\n",
      "SSH-Patator 5098\n",
      "(5098, 75)\n",
      "----------------------------------------\n",
      "Bot 1954\n",
      "(1954, 75)\n",
      "----------------------------------------\n",
      "Web Attack Brute Force 1507\n",
      "(1507, 75)\n",
      "----------------------------------------\n",
      "Web Attack XSS 652\n",
      "(652, 75)\n",
      "----------------------------------------\n",
      "Infiltration 36\n",
      "(36, 75)\n",
      "----------------------------------------\n",
      "Web Attack Sql Injection 21\n",
      "(21, 75)\n",
      "----------------------------------------\n",
      "Heartbleed 11\n",
      "(11, 75)\n",
      "----------------------------------------\n",
      "========================================\n",
      "Label\n",
      "BENIGN                      20000\n",
      "DoS Hulk                    20000\n",
      "DDoS                        20000\n",
      "PortScan                    20000\n",
      "DoS GoldenEye               10286\n",
      "FTP-Patator                  6878\n",
      "DoS slowloris                5692\n",
      "DoS Slowhttptest             5263\n",
      "SSH-Patator                  5098\n",
      "Bot                          1954\n",
      "Web Attack Brute Force       1507\n",
      "Web Attack XSS                652\n",
      "Infiltration                   36\n",
      "Web Attack Sql Injection       21\n",
      "Heartbleed                     11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "MAX_SAMPLE_SIZE = 20000\n",
    "\n",
    "counts = df[\"Label\"].value_counts()\n",
    "new_df = pd.DataFrame()\n",
    "for label, count in counts.items():\n",
    "    print(label, count)\n",
    "    if count > MAX_SAMPLE_SIZE:\n",
    "        # under sampling\n",
    "        tmp = under_sampling(df[df[\"Label\"] == label], MAX_SAMPLE_SIZE)\n",
    "        print(tmp.shape)\n",
    "    else:\n",
    "        # over sampling\n",
    "        tmp = over_sampling(df[df[\"Label\"] == label])\n",
    "        print(tmp.shape)\n",
    "    print(\"--\" * 20)\n",
    "    new_df = pd.concat([new_df, tmp])\n",
    "\n",
    "print(\"==\" * 20)\n",
    "print(new_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "BENIGN                      20000\n",
      "DoS Hulk                    20000\n",
      "DDoS                        20000\n",
      "PortScan                    20000\n",
      "DoS GoldenEye               10286\n",
      "FTP-Patator                  6878\n",
      "DoS slowloris                5692\n",
      "DoS Slowhttptest             5263\n",
      "SSH-Patator                  5098\n",
      "Bot                          1954\n",
      "Web Attack Brute Force       1507\n",
      "Web Attack XSS                652\n",
      "Infiltration                   36\n",
      "Web Attack Sql Injection       21\n",
      "Heartbleed                     11\n",
      "Name: count, dtype: int64\n",
      "BENIGN 20000\n",
      "(20000, 75)\n",
      "DoS Hulk 20000\n",
      "(20000, 75)\n",
      "DDoS 20000\n",
      "(20000, 75)\n",
      "PortScan 20000\n",
      "(20000, 75)\n",
      "DoS GoldenEye 10286\n",
      "(10286, 75)\n",
      "FTP-Patator 6878\n",
      "(6878, 75)\n",
      "DoS slowloris 5692\n",
      "(5692, 75)\n",
      "DoS Slowhttptest 5263\n",
      "(5263, 75)\n",
      "SSH-Patator 5098\n",
      "(5098, 75)\n",
      "Bot 1954\n",
      "(1954, 75)\n",
      "Web Attack Brute Force 1507\n",
      "(1507, 75)\n",
      "Web Attack XSS 652\n",
      "(652, 75)\n",
      "Infiltration 36\n",
      "(36, 75)\n",
      "Web Attack Sql Injection 21\n",
      "(21, 75)\n",
      "Heartbleed 11\n",
      "(11, 75)\n"
     ]
    }
   ],
   "source": [
    "print(new_df[\"Label\"].value_counts())\n",
    "\n",
    "counts = new_df[\"Label\"].value_counts()\n",
    "# save to csv (separate by label)\n",
    "for label, count in counts.items():\n",
    "    print(label, count)\n",
    "    tmp = new_df[new_df[\"Label\"] == label]\n",
    "    print(tmp.shape)\n",
    "\n",
    "    label = label.replace(\" \", \"_\")\n",
    "    # save to csv\n",
    "    tmp.to_csv(f\"{DIR_PATH}/train/{label}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")\n",
    "test.to_csv(f\"{DIR_PATH}/test/test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
