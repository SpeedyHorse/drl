{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import ipaddress as ip\n", "import pandas as pd\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from glob import glob\n", "from datetime import datetime\n", "# train_test_split\n", "from sklearn.model_selection import train_test_split\n", "import scipy\n", "import os\n", "import csv\n", "# ENN\n", "from imblearn.under_sampling import EditedNearestNeighbours\n", "# K-Means\n", "from sklearn.cluster import KMeans"]}, {"cell_type": "markdown", "metadata": {}, "source": ["WGAN"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "from torch.utils.data import DataLoader, Dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DIR_PATH = \"./data\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_csv_files(dir_name_list):\n", "    for _dir in dir_name_list:\n", "        paths = glob(f\"{DIR_PATH}/raw/*.csv\")\n", "        \n", "        for path in paths:\n", "            print(path)\n", "            filename = os.path.basename(path)\n", "            read_write(path, f\"{DIR_PATH}/raw_after/{_dir[0]}/{filename}\", _dir[2])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def read_write(path, write_path, labels=None):\n", "    with open(path, \"r\", encoding=\"utf-8\") as inputs, open(write_path, \"w\", newline=\"\") as outputs:\n", "        reader = csv.reader(inputs)\n", "        writer = csv.writer(outputs)\n", "        headers = next(reader)\n", "        if not labels:\n", "            headers = [ head.strip() for head in headers ]\n", "            writer.writerow(headers)\n", "        else:\n", "            writer.writerow(labels)\n", "        for row in reader:\n", "            if row == headers:\n", "                break\n", "            writer.writerow(row)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["datasets = [\n", "    [\"cicids2017\", \"BENIGN\", None],\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["get_csv_files(datasets)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["files = glob(f\"{DIR_PATH}/raw_after/cicids2017/*.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.DataFrame()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for file in files:\n", "    if \"label_\" in file:\n", "        continue\n", "    tmp = pd.read_csv(file)\n", "    df = pd.concat([df, tmp])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(df[\"Label\"].unique())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DROP_COLUMNS = [\n", "    \"Flow ID\",\n", "    \"Source IP\",\n", "    \"Source Port\",\n", "    \"Bwd PSH Flags\",\n", "    \"Bwd URG Flags\",\n", "    \"Fwd Avg Bytes/Bulk\",\n", "    \"Fwd Avg Packets/Bulk\",\n", "    \"Fwd Avg Bulk Rate\",\n", "    \"Bwd Avg Bytes/Bulk\",\n", "]\n", "print(len(df))\n", "df = df.drop(columns=DROP_COLUMNS).replace([np.inf, -np.inf], np.nan)\n", "df = df.dropna(how=\"any\").dropna(how=\"all\", axis=1).drop_duplicates()\n", "print(len(df))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[4]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for value in df[\"Destination IP\"].unique()[:5]:\n", "    ip_int = int(ip.IPv4Address(value))\n", "    print(value, ip_int)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DATE_FORMAT = \"%m/%d/%Y %H:%M\"\n", "for value in df[\"Timestamp\"].unique()[:5]:\n", "    dt = datetime.strptime(value, DATE_FORMAT)\n", "    dt = dt.timestamp()\n", "    print(value, \"|\", dt)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Timestamp -> int"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df[\"continuous_time\"] = df[\"Timestamp\"].apply(\n", "    lambda x: datetime.strptime(x, DATE_FORMAT).timestamp()\n", ")\n", "df = df.drop(columns=[\"Timestamp\"])\n", "# rename: continuous_time -> timestamp\n", "df = df.rename(columns={\"continuous_time\": \"Timestamp\"})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Destination IP -> int"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df[\"destination_ip\"] = df[\"Destination IP\"].apply(\n", "    lambda x: int(ip.IPv4Address(x))\n", ")\n", "df = df.drop(columns=[\"Destination IP\"])\n", "# rename: destination_ip -> Destination IP\n", "df = df.rename(columns={\"destination_ip\": \"Destination IP\"})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["print(\"wgan running?: \", end=\"\")<br>\n", "result = input()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["if result != \"y\":"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if False:\n", "    counts = df[\"Label\"].value_counts()\n", "    for (label, count) in counts.items():\n", "        label_ed = label.strip().replace(\" \", \"_\")\n", "        df[df[\"Label\"] == label].to_csv(f\"{DIR_PATH}/raw_after/cicids2017/label_{label_ed}.csv\", index=False)\n", "    print(\"next: wgan running\")\n", "    exit()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[7]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def min_max(dataframe):\n", "    for column in dataframe.columns:\n", "        # print(dataframe[column].unique())\n", "        if len(dataframe[column].unique()) <= 1:\n", "            # print(f\"Column {column} has only one unique value. Skipping normalization.\")\n", "            continue\n", "        dataframe[column] = (\n", "            dataframe[column] - dataframe[column].min()\n", "        ) / (dataframe[column].max() - dataframe[column].min())\n", "    return dataframe"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def normalize(dataframe):\n", "    categorical_frame = dataframe[[\"Destination Port\", \"Protocol\", \"Label\"]]\n", "    other_frame = dataframe.drop(columns=[\"Destination Port\", \"Protocol\", \"Label\"])\n\n", "    # normalize\n", "    normalized_frame = min_max(other_frame)\n", "    normalized_frame = normalized_frame.replace([np.inf, -np.inf], np.nan)\n", "    tmp = pd.concat(\n", "        [normalized_frame, categorical_frame], axis=1\n", "    )\n", "    return tmp"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def under_sampling(dataframe, size):\n", "    CLUSTER_SIZE = 1\n\n", "    # normalize\n", "    normalized_frame = normalize(dataframe)\n", "    print(\"normalized_frame\", normalized_frame.shape)\n", "    normalized_frame = normalized_frame.replace([np.inf, -np.inf], np.nan)\n", "    continuous_frame = normalized_frame.drop(columns=[\"Destination Port\", \"Protocol\", \"Label\"])\n", "    kmeans = KMeans(n_clusters=CLUSTER_SIZE, random_state=0)\n", "    print(\"k-means: start\")\n", "    cluster_labels = kmeans.fit_predict(continuous_frame)\n", "    print(\"k-means: end\")\n", "    i = cluster_labels[0]\n", "    cluster_indices = np.where(cluster_labels == i)[0]\n", "    distances = np.linalg.norm(\n", "        continuous_frame.iloc[cluster_indices] - kmeans.cluster_centers_[i], axis=1\n", "    )\n", "    closest_indices = cluster_indices[np.argsort(distances)[:size]]\n", "    return normalized_frame.iloc[closest_indices]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["FEATURE_DIM = 74\n", "LATENT_DIM = 100"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class FlowDataset(Dataset):\n", "    def __init__(self, dataframe, transform=None):\n", "        self.data_frame = dataframe\n", "        \n", "        # self.features = self.data_frame.drop(columns=[\"Label\"])\n", "        self.features = self.features.select_dtypes(include=[np.number])\n", "        self.transform = transform\n", "    def __len__(self):\n", "        return len(self.data_frame)\n", "    \n", "    def __getitem__(self, idx):\n", "        features = self.features.iloc[idx].values.astype(np.float32)\n", "        return torch.tensor(features)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Generator(nn.Module):\n", "    def __init__(self):\n", "        super(Generator, self).__init__()\n", "        self.model = nn.Sequential(\n", "            nn.Linear(LATENT_DIM, 256),\n", "            nn.LeakyReLU(0.2),\n", "            nn.Linear(256, 512),\n", "            nn.LeakyReLU(0.2),\n", "            nn.Linear(512, 1024),\n", "            nn.LeakyReLU(0.2),\n", "            nn.Linear(1024, FEATURE_DIM),\n", "            nn.Sigmoid()\n", "        )\n", "    \n", "    def forward(self, z):\n", "        return self.model(z)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Critic(nn.Module):\n", "    def __init__(self):\n", "        super(Critic, self).__init__()\n", "        self.model = nn.Sequential(\n", "            nn.Linear(FEATURE_DIM, 512),\n", "            nn.LeakyReLU(0.2),\n", "            nn.Linear(512, 256),\n", "            nn.LeakyReLU(0.2),\n", "            nn.Linear(256, 128),\n", "            nn.LeakyReLU(0.2),\n", "            nn.Linear(128, 1)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        return self.model(x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["generator = Generator()\n", "critic = Critic()\n", "generator_optimizer = optim.RMSprop(generator.parameters(), lr=0.0001)\n", "critic_optimizer = optim.RMSprop(critic.parameters(), lr=0.0001)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def over_sampling(dataframe):\n", "    dataframe = normalize(dataframe)\n", "    size = 20000 - len(dataframe)\n", "    before_label = dataframe[\"Label\"].unique()[0]\n", "    label = before_label.replace(\" \", \"_\")\n", "    \n", "    checkpoint = torch.load(f\"{DIR_PATH}/wgan/model/wgan_cicflowmeter_label_{label}.pth\")\n", "    generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n", "    critic.load_state_dict(checkpoint[\"critic_state_dict\"])\n", "    generator_optimizer.load_state_dict(checkpoint[\"generator_optimizer_state_dict\"])\n", "    critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer_state_dict\"])\n", "    generator.eval()\n", "    with torch.no_grad():\n", "        z = torch.randn(size, LATENT_DIM)\n", "        generated_data = generator(z).numpy()\n", "    generated_df = pd.DataFrame(generated_data, columns=dataframe.columns[:-1])\n", "    generated_df[\"Label\"] = before_label\n", "    \n", "#     print(dataframe.columns)\n", "#     print(generated_df.columns)\n", "    convined_df = pd.concat([dataframe, generated_df])\n", "    return convined_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["MAX_SAMPLE_SIZE = 20000"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["counts = df[\"Label\"].value_counts()\n", "new_df = pd.DataFrame()\n", "for label, count in counts.items():\n", "    print(label, count)\n", "    if count > MAX_SAMPLE_SIZE:\n", "        # under sampling\n", "        tmp = under_sampling(df[df[\"Label\"] == label], MAX_SAMPLE_SIZE)\n", "        print(tmp.shape)\n", "    else:\n", "        # over sampling\n", "        tmp = over_sampling(df[df[\"Label\"] == label])\n", "        print(tmp.shape)\n", "        # print(tmp[~tmp.index.duplicated(keep=\"first\")])\n", "        # print(new_df[~new_df.index.duplicated(keep=\"first\")])\n", "    print(\"--\" * 20)\n", "    tmp = tmp.reset_index(drop=True)\n", "    new_df = new_df.reset_index(drop=True)\n", "    new_df = pd.concat([new_df, tmp], ignore_index=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"==\" * 20)\n", "print(new_df[\"Label\"].value_counts())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(new_df[\"Label\"].value_counts())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["counts = new_df[\"Label\"].value_counts()\n", "# save to csv (separate by label)\n", "for label, count in counts.items():\n", "    print(label, count)\n", "    tmp = new_df[new_df[\"Label\"] == label]\n", "    print(tmp.shape)\n", "    label = label.replace(\" \", \"_\")\n", "    # save to csv\n", "    tmp.to_csv(f\"{DIR_PATH}/train/{label}.csv\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["_, test = train_test_split(\n", "    df,\n", "    test_size=0.3,\n", "    random_state=42,\n", ")\n", "test.to_csv(f\"{DIR_PATH}/test/test.csv\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}